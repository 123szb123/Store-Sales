{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "18898378",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.deterministic import DeterministicProcess\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from scipy.signal import periodogram\n",
    "from statsmodels.tsa.deterministic import DeterministicProcess, CalendarFourier # For modeling trends and seasonal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4b650990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000888 entries, 0 to 3000887\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Dtype  \n",
      "---  ------       -----  \n",
      " 0   id           int64  \n",
      " 1   date         object \n",
      " 2   store_nbr    int64  \n",
      " 3   family       object \n",
      " 4   sales        float64\n",
      " 5   onpromotion  int64  \n",
      "dtypes: float64(1), int64(3), object(2)\n",
      "memory usage: 137.4+ MB\n",
      "None\n",
      "\n",
      "\n",
      "STORES METADATA \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 54 entries, 0 to 53\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   store_nbr  54 non-null     int64 \n",
      " 1   city       54 non-null     object\n",
      " 2   state      54 non-null     object\n",
      " 3   type       54 non-null     object\n",
      " 4   cluster    54 non-null     int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 2.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Datasets\n",
    "train = pd.read_csv('data/train.csv')\n",
    "stores = pd.read_csv('data/stores.csv')\n",
    "\n",
    "print('TRAINING DATA \\n')\n",
    "print(train.info())\n",
    "print('\\n')\n",
    "print('STORES METADATA \\n')\n",
    "print(stores.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "268dd58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date strings to datetime objects\n",
    "train['date'] = pd.to_datetime(train['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "75d004e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000888 entries, 0 to 3000887\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Dtype         \n",
      "---  ------       -----         \n",
      " 0   id           int64         \n",
      " 1   date         datetime64[ns]\n",
      " 2   store_nbr    int64         \n",
      " 3   family       object        \n",
      " 4   sales        float64       \n",
      " 5   onpromotion  int64         \n",
      " 6   city         object        \n",
      " 7   state        object        \n",
      " 8   type         object        \n",
      " 9   cluster      int64         \n",
      "dtypes: datetime64[ns](1), float64(1), int64(4), object(4)\n",
      "memory usage: 228.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.merge(\n",
    "    train,\n",
    "    stores,\n",
    "    on = 'store_nbr',\n",
    "    how = 'left'\n",
    ")\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "21a29180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we drop id feature but only if it exists\n",
    "if 'id' in df.columns:\n",
    "    df = df.drop(columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "748d4cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values('date')\n",
    "\n",
    "df['dow'] = df['date'].dt.dayofweek # 0=mon 1=tue 2=wed 3=thu 4=fri 5=sat 6=sun because monday â‰  saturday in sales\n",
    "df['month'] = df['date'].dt.month\n",
    "df['year'] = df['date'].dt.year\n",
    "df['is_weekend'] = (df['dow'] >= 5).astype(int) # because weekends are spikes\n",
    "df['time_idx'] = (df['date'] - df['date'].min()).dt.days  # linear time trend\n",
    "\n",
    "df = df.sort_values(['store_nbr','family','date']) # add lag features\n",
    "\n",
    "df['lag7'] = df.groupby(['store_nbr','family'])['sales'].shift(7) # 1 week lag to capture weekly trends\n",
    "df['roll7'] = ( # smooth it out by finding average behavior over past week\n",
    "    df.groupby(['store_nbr','family'])['sales']\n",
    "      .shift(1)\n",
    "      .rolling(7)\n",
    "      .mean()\n",
    ") # 7 day rolling mean gets lagged by 1 day\n",
    "\n",
    "# now drop rows where lag features are null or nan (the first ~7 days per store/family)\n",
    "df_model = df.dropna(subset=['lag7','roll7']).copy()\n",
    "\n",
    "# okay so apparently this works because previously the mmodel had no clue that demand is always high on specific dates (like december 23 or 24)\n",
    "# so it was underpredicting peaks and overpredicting quiet days. this essentially teaches the model the calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "699be52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\n",
    "    'store_nbr',\n",
    "    'family',\n",
    "    'city',\n",
    "    'state',\n",
    "    'type',\n",
    "    'cluster',\n",
    "    'dow',\n",
    "    'month'\n",
    "]\n",
    "\n",
    "numeric_features = [\n",
    "    'onpromotion',  # onpromotion was previously treated ascategorical so changed it\n",
    "    'lag7',\n",
    "    'roll7', \n",
    "    'is_weekend',\n",
    "    'time_idx' # linear time trend which is from 0 to the length of the dataset\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "83a390a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ]\n",
    ") # preprocess numeric features with standard scaler and categorical with one hot encoder\n",
    "\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', Ridge(alpha=1.0))\n",
    "]) # what this does is first preprocess the data then eventually fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "10539b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('sales',axis=1)\n",
    "y = df['sales']\n",
    "\n",
    "from sklearn.model_selection import train_test_split # split data into training and cross-validation sets\n",
    "\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X,y, test_size=0.3, random_state=57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bdb0d343",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df_model.sort_values('date') # ensure chronological order\n",
    "\n",
    "feature_cols = categorical_features + numeric_features # columns we want the model to see\n",
    "\n",
    "X_all = df_model[feature_cols]\n",
    "y_all = df_model['sales']\n",
    "dates = df_model['date']\n",
    "\n",
    "cutoff_date = dates.quantile(0.8) # 80 percent cutoff date for training vs validation (apparently 80 is a good number)\n",
    "\n",
    "# so if we had 100 days of data, the first 80 days would be training and last 20 days would be used for validation\n",
    "\n",
    "train_mask = dates <= cutoff_date # training data is dates on or before cutoff date so the first ~80%\n",
    "cv_mask    = dates > cutoff_date # cross-validation data is dates after cutoff date so the last ~20%\n",
    "\n",
    "X_train = X_all[train_mask] # these are training features\n",
    "y_train = y_all[train_mask] # training targets \n",
    "# these are passed into fit function of model_pipeline\n",
    "\n",
    "X_cv = X_all[cv_mask] # cross-validation features which we will pass into predict function\n",
    "y_cv = y_all[cv_mask] # now we got the cross-validation targets to pass into rsmle function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1cb82751",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline.fit(X_train,y_train)\n",
    "predictions = model_pipeline.predict(X_cv)\n",
    "predictions = np.maximum(predictions,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4599b906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[predictions < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a4307ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSLE:  1.4747\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import root_mean_squared_log_error\n",
    "\n",
    "rmsle = root_mean_squared_log_error(y_cv,predictions)\n",
    "print(f'RMSLE: {rmsle: .4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fb2356",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b38b1dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSLE: 0.70\n",
      "\n",
      "Forecast:\n",
      "         date      forecast\n",
      "0  2017-08-16  9.202675e+05\n",
      "1  2017-08-17  8.767225e+05\n",
      "2  2017-08-18  8.067180e+05\n",
      "3  2017-08-19  8.109391e+05\n",
      "4  2017-08-20  8.568684e+05\n",
      "5  2017-08-21  9.135865e+05\n",
      "6  2017-08-22  9.166447e+05\n",
      "7  2017-08-23  8.865838e+05\n",
      "8  2017-08-24  8.524070e+05\n",
      "9  2017-08-25  7.877709e+05\n",
      "10 2017-08-26  7.837810e+05\n",
      "11 2017-08-27  8.320980e+05\n",
      "12 2017-08-28  9.219839e+05\n",
      "13 2017-08-29  9.608057e+05\n",
      "14 2017-08-30  9.597390e+05\n",
      "15 2017-08-31  1.086321e+06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewpiela/Python Projects/ml_marathon_store_sales/.venv/lib/python3.13/site-packages/statsmodels/tsa/deterministic.py:308: UserWarning: Only PeriodIndexes, DatetimeIndexes with a frequency set, RangesIndexes, and Index with a unit increment support extending. The index is set will contain the position relative to the data length.\n",
      "  fcast_index = self._extend_index(index, steps, forecast_index)\n",
      "/Users/andrewpiela/Python Projects/ml_marathon_store_sales/.venv/lib/python3.13/site-packages/statsmodels/tsa/deterministic.py:441: UserWarning: Only PeriodIndexes, DatetimeIndexes with a frequency set, RangesIndexes, and Index with a unit increment support extending. The index is set will contain the position relative to the data length.\n",
      "  fcast_index = self._extend_index(index, steps, forecast_index)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tot_sales = df.groupby('date')['sales'].sum().reset_index()\n",
    "tot_sales = tot_sales.set_index('date').asfreq('D').fillna(0)\n",
    "\n",
    "tot_sales = tot_sales.reset_index()  \n",
    "tot_sales['date'] = pd.to_datetime(tot_sales['date'])\n",
    "tot_sales = tot_sales.set_index('date').sort_index()\n",
    "\n",
    "dp = DeterministicProcess(\n",
    "    index = tot_sales.index,\n",
    "    constant=True,\n",
    "    order=1, # Linear trend\n",
    "    seasonal=True,\n",
    "    period=365, # Yearly seasonality\n",
    "    drop=True\n",
    ")\n",
    "\n",
    "# Training features\n",
    "X_train = dp.in_sample()\n",
    "y = tot_sales['sales']\n",
    "\n",
    "# Fit the linear model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train,y)\n",
    "\n",
    "# Make predictions on training data, for evaluation\n",
    "y_pred = model.predict(X_train)\n",
    "\n",
    "y_pred = np.maximum(y_pred, 0) # forces no negative predictions\n",
    "\n",
    "# Calculate training error (root mean squared error)\n",
    "rmsle = np.sqrt(mean_squared_log_error(y,y_pred))\n",
    "print(f'Training RMSLE: {rmsle:.2f}')\n",
    "\n",
    "# Out-of-sample predictions\n",
    "forecast_steps = 16\n",
    "X_forecast = dp.out_of_sample(steps=forecast_steps)\n",
    "forecast = model.predict(X_forecast)\n",
    "\n",
    "# forecast dataframe\n",
    "last_date = tot_sales.index[-1]\n",
    "forecast_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), \n",
    "                                periods=forecast_steps, \n",
    "                                freq='D')\n",
    "\n",
    "forecast_df = pd.DataFrame({\n",
    "    'date' : forecast_dates,\n",
    "    'forecast': forecast\n",
    "})\n",
    "\n",
    "print(\"\\nForecast:\")\n",
    "print(forecast_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
